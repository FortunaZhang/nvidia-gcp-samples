{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GCP Project\n",
    "### Assume you already have a GCS path set up as below\n",
    "<pre>\n",
    "[YOUR GCS Path - gs://dlvm-dataset/bert]    \n",
    "      ├──────────  checkpoint \n",
    "      │               └─── bert_tf_v1_1_large_fp16_384_v2\n",
    "      │\n",
    "      ├──────────  output   \n",
    "      ├──────────  squad  \n",
    "      ├──────────  trt_engine  \n",
    "      └──────────  trt_deployment  \n",
    "                      └─── bert\n",
    "                             └─── 1\n",
    "</pre>\n",
    "\n",
    "This notebook demonstrate a 3 steps workflow, including Finetune, TRT optimization, Triton Model Server. For user who interest in CAIP Prediction custom container integration with Triton, please skip the first 2 steps.\n",
    "\n",
    "Container image for the first 2 steps are included in the repo as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=k80-exploration\n",
      "env: MODEL_GCS_PATH=gs://dlvm-dataset/bert/trt_deployment\n",
      "env: ENDPOINT=https://alpha-ml.googleapis.com/v1\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID=[YOUR GCP Project]\n",
    "%env MODEL_GCS_PATH=[YOUR GCS Path] \n",
    "%env ENDPOINT=https://alpha-ml.googleapis.com/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching NGC BERT Fine Tuning Training Job in AI Platform Training Custom Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"FINE_TUNE_JOB_NAME\"] = \"bert_finetuning_0001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training $FINE_TUNE_JOB_NAME \\\n",
    "    --master-image-uri gcr.io/$Project_ID/tf_bert_gcsfuse:latest \\\n",
    "    --region us-central1 \\\n",
    "    --master-accelerator count=8,type=nvidia-tesla-v100 \\\n",
    "    --master-machine-type n1-highmem-96 \\\n",
    "    --scale-tier custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs stream-logs $FINE_TUNE_JOB_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching NGC TensorRT container to Optimize TF checkpoint to TRT Engine in AI Platform Training Custom Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TRT_JOB_NAME\"] = \"bert_trt_123129\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --master-image-uri gcr.io/$Project_ID/bert_trt_gcsfuse:latest \\\n",
    "    --region us-central1 \\\n",
    "    --master-accelerator count=1,type=nvidia-tesla-t4 \\\n",
    "    --master-machine-type n1-highmem-8 \\\n",
    "    --scale-tier custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai-platform jobs stream-logs $JOB_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://dlvm-dataset/bert/trt_engine/bert_large_384_int8.engine \\\n",
    "    gs://dlvm-dataset/bert/trt_deployment/bert/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying NVIDIA Triton Inference Server in AI Platform Prediction Custom Container (REST API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will walk through the process of deploying NVIDIA's Triton Inference Server into AI Platform Prediction Custom Container service in the Direct Model Server mode:\n",
    "\n",
    "![](img/caip_triton_container_diagram_direct.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and deploy Model and Model Version\n",
    "\n",
    "In this section, we will deploy the BERT large QA TensorRT Engine that optimized for INT8 on T4 to AI platform prediction custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT TRT Model\n",
    "\n",
    "#### Create Model\n",
    "\n",
    "AI Platform Prediction uses a Model/Model Version Hierarchy, where the Model is a logical grouping of Model Versions.  We will first create the Model.\n",
    "\n",
    "Because the MODEL_NAME variable will be used later to specify the predict route, and Triton will use that route to run prediction on a specific model, we must set the value of this variable to a valid name of a model.  For this section, will use the \"simple\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_NAME=bert\n"
     ]
    }
   ],
   "source": [
    "%env MODEL_NAME=bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": {\n",
      "    \"code\": 409,\n",
      "    \"message\": \"Field: model.name Error: A model with the same name already exists.\",\n",
      "    \"status\": \"ALREADY_EXISTS\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.BadRequest\",\n",
      "        \"fieldViolations\": [\n",
      "          {\n",
      "            \"field\": \"model.name\",\n",
      "            \"description\": \"A model with the same name already exists.\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X \\\n",
    "    POST -k -H \"Content-Type: application/json\" \\\n",
    "    -d \"{'name': '\"$MODEL_NAME\"'}\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Version\n",
    "\n",
    "After the Model is created, we can now create a Model Version under this Model.  Each Model Version will need a name that is unique within the Model.  In AI Platform Prediction Custom Container, a {Project}/{Model}/{ModelVersion} uniquely identifies the specific container and model artifact used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: VERSION_NAME=vdongm02\n",
      "env: TRITON_MODEL_NAME=bert\n"
     ]
    }
   ],
   "source": [
    "%env VERSION_NAME=vdongm02\n",
    "%env TRITON_MODEL_NAME=bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following specifications tell AI Platform how to create the Model Version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "triton_bert_version = {\n",
    "  \"name\": os.getenv(\"VERSION_NAME\"),\n",
    "  \"deployment_uri\": os.getenv(\"MODEL_GCS_PATH\"),\n",
    "  \"container\": {\n",
    "    \"image\": \"gcr.io/\"+os.getenv(\"PROJECT_ID\")+\"/tritonserver:20.08-py3\",\n",
    "    \"args\": [\"tritonserver\",\n",
    "             \"--model-repository=$(AIP_STORAGE_URI)\",\n",
    "             \"--strict-model-config=false\"\n",
    "    ],\n",
    "    \"env\": [\n",
    "    ], \n",
    "    \"ports\": [\n",
    "      { \"containerPort\": 8000 }\n",
    "    ]\n",
    "  },\n",
    "  \"routes\": {\n",
    "    \"predict\": \"/v2/models/\"+os.getenv(\"TRITON_MODEL_NAME\")+\"/infer\",\n",
    "    \"health\": \"/v2/models/\"+os.getenv(\"TRITON_MODEL_NAME\")\n",
    "  },\n",
    "  \"machine_type\": \"n1-standard-4\",\n",
    "  \"acceleratorConfig\": {\n",
    "    \"count\":1,\n",
    "    \"type\":\"nvidia-tesla-t4\"\n",
    "  },\n",
    "  \"autoScaling\": {\n",
    "    \"minNodes\": 1\n",
    "  }\n",
    "}\n",
    "\n",
    "with open(\"triton_bert_version.json\", \"w\") as f: \n",
    "  json.dump(triton_bert_version, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"projects/k80-exploration/operations/create_bert_vdongm02-1600377033412\",\n",
      "  \"metadata\": {\n",
      "    \"@type\": \"type.googleapis.com/google.cloud.ml.v1.OperationMetadata\",\n",
      "    \"createTime\": \"2020-09-17T21:10:34Z\",\n",
      "    \"operationType\": \"CREATE_VERSION\",\n",
      "    \"modelName\": \"projects/k80-exploration/models/bert\",\n",
      "    \"version\": {\n",
      "      \"name\": \"projects/k80-exploration/models/bert/versions/vdongm02\",\n",
      "      \"deploymentUri\": \"gs://dlvm-dataset/bert/trt_deployment\",\n",
      "      \"createTime\": \"2020-09-17T21:10:33Z\",\n",
      "      \"autoScaling\": {\n",
      "        \"minNodes\": 1\n",
      "      },\n",
      "      \"etag\": \"XbXqCY1HEiI=\",\n",
      "      \"machineType\": \"n1-standard-4\",\n",
      "      \"acceleratorConfig\": {\n",
      "        \"count\": \"1\",\n",
      "        \"type\": \"NVIDIA_TESLA_T4\"\n",
      "      },\n",
      "      \"container\": {\n",
      "        \"image\": \"gcr.io/k80-exploration/tritonserver:20.08-py3\",\n",
      "        \"args\": [\n",
      "          \"tritonserver\",\n",
      "          \"--model-repository=$(AIP_STORAGE_URI)\",\n",
      "          \"--strict-model-config=false\"\n",
      "        ],\n",
      "        \"ports\": [\n",
      "          {\n",
      "            \"containerPort\": 8000\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"routes\": {\n",
      "        \"predict\": \"/v2/models/bert/infer\",\n",
      "        \"health\": \"/v2/models/bert\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X \\\n",
    "    POST -k -H \"Content-Type: application/json\" \\\n",
    "    -d @triton_bert_version.json \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/${MODEL_NAME}/versions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the status of Model Version creation\n",
    "\n",
    "Creating a Model Version may take several minutes.  You can check on the status of this specfic Model Version with the following, and a successful deployment will show:\n",
    "\n",
    "`\"state\": \"READY\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: VERSION_NAME=dongm01\n"
     ]
    }
   ],
   "source": [
    "%env VERSION_NAME=dongm01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"projects/k80-exploration/models/bert/versions/dongm01\",\n",
      "  \"deploymentUri\": \"gs://dlvm-dataset/bert/trt_deployment\",\n",
      "  \"createTime\": \"2020-09-17T03:16:54Z\",\n",
      "  \"lastUseTime\": \"2020-09-17T20:58:45Z\",\n",
      "  \"autoScaling\": {\n",
      "    \"minNodes\": 1\n",
      "  },\n",
      "  \"state\": \"READY\",\n",
      "  \"etag\": \"73gEAhBaORs=\",\n",
      "  \"machineType\": \"n1-standard-4\",\n",
      "  \"acceleratorConfig\": {\n",
      "    \"count\": \"1\",\n",
      "    \"type\": \"NVIDIA_TESLA_T4\"\n",
      "  },\n",
      "  \"container\": {\n",
      "    \"image\": \"gcr.io/k80-exploration/tritonserver:20.08-py3\",\n",
      "    \"args\": [\n",
      "      \"tritonserver\",\n",
      "      \"--model-repository=$(AIP_STORAGE_URI)\",\n",
      "      \"--strict-model-config=false\"\n",
      "    ],\n",
      "    \"ports\": [\n",
      "      {\n",
      "        \"containerPort\": 8000\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"routes\": {\n",
      "    \"predict\": \"/v2/models/bert/infer\",\n",
      "    \"health\": \"/v2/models/bert\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X GET -k -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/${MODEL_NAME}/versions/${VERSION_NAME}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To list all Model Versions and their states in this Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET -k -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    \"${ENDPOINT}/projects/${PROJECT_ID}/models/${MODEL_NAME}/versions/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Prediction\n",
    "\n",
    "[TODO] add the basic description of preprocessing and post processing with bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.create_squad_data import read_squad_examples, convert_examples_to_features\n",
    "\n",
    "import os, requests, ast\n",
    "import tokenization\n",
    "import tensorflow as tf\n",
    "\n",
    "from get_request_body_bert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bert_config():\n",
    "    \"\"\"\n",
    "    Defines the configuration of BERT model\n",
    "    \"\"\"\n",
    "    global do_lower_case \n",
    "    global predict_batch_size\n",
    "    global max_seq_length\n",
    "    global doc_stride \n",
    "    global max_query_length \n",
    "    global verbose_logging \n",
    "    global version_2_with_negative \n",
    "    global n_best_size\n",
    "    global max_answer_length\n",
    "\n",
    "    # Set True for uncased model\n",
    "    do_lower_case = True\n",
    "\n",
    "    # Total batch size for predictions\n",
    "    predict_batch_size = 1\n",
    "\n",
    "    # The maximum total input sequence length after WordPiece tokenization. \n",
    "    # Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "    max_seq_length = 384\n",
    "\n",
    "    # When splitting up a long document into chunks, how much stride to take between chunks.\n",
    "    doc_stride = 128\n",
    "\n",
    "    # The maximum number of tokens for the question. \n",
    "    # Questions longer than this will be truncated to this length.\n",
    "    max_query_length = 64\n",
    "\n",
    "    # Set True for verbosity\n",
    "    verbose_logging = True\n",
    "\n",
    "    # Set True if the dataset has samples with no answers. For SQuAD 1.1, this is set to False\n",
    "    version_2_with_negative = False\n",
    "\n",
    "    # The total number of n-best predictions to generate in the nbest_predictions.json output file.\n",
    "    n_best_size = 20\n",
    "\n",
    "    # The maximum length of an answer that can be generated. \n",
    "    # This is needed  because the start and end predictions are not conditioned on one another.\n",
    "    max_answer_length = 30\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_bert_config()\n",
    "\n",
    "input_data = [{\"paragraphs\": \n",
    "                   [{\"context\":\n",
    "                         \"\"\"TensorRT is a high performance deep learning inference platform \n",
    "                         that delivers low latency and high throughput for apps such as \n",
    "                         recommenders, speech and image/video on NVIDIA GPUs. It includes \n",
    "                         parsers to import models, and plugins to support novel ops and \n",
    "                         layers before applying optimizations for inference. Today NVIDIA \n",
    "                         is open-sourcing parsers and plugins in TensorRT so that the deep \n",
    "                         learning community can customize and extend these components to \n",
    "                         take advantage of powerful TensorRT optimizations for your apps.\"\"\", \n",
    "                     \"qas\": [{ \n",
    "                         \"question\": \"What is TensorRT?\", \n",
    "                         \"id\": \"Q1\"}]}]}]\n",
    "\n",
    "# input_data = [{\"paragraphs\":\n",
    "#                    [{\"context\":\n",
    "#                          \"\"\"The Apollo program, also known as Project Apollo, was the third \n",
    "#                          United States human spaceflight program carried out by the National \n",
    "#                          Aeronautics and Space Administration (NASA), which accomplished \n",
    "#                          landing the first humans on the Moon from 1969 to 1972. First \n",
    "#                          conceived during Dwight D. Eisenhower's administration as a \n",
    "#                          three-man spacecraft to follow the one-man Project Mercury which \n",
    "#                          put the first Americans in space, Apollo was later dedicated to \n",
    "#                          President John F. Kennedy's national goal of landing a man on \n",
    "#                          the Moon and returning him safely to the Earth by the end of the \n",
    "#                          1960s, which he proposed in a May 25, 1961, address to Congress. \n",
    "#                          Project Mercury was followed by the two-man Project Gemini. \n",
    "#                          The first manned flight of Apollo was in 1968. Apollo ran from \n",
    "#                          1961 to 1972, and was supported by the two man Gemini program \n",
    "#                          which ran concurrently with it from 1962 to 1966. Gemini missions \n",
    "#                          developed some of the space travel techniques that were necessary \n",
    "#                          for the success of the Apollo missions. Apollo used Saturn family \n",
    "#                          rockets as launch vehicles. Apollo/Saturn vehicles were also used \n",
    "#                          for an Apollo Applications Program, which consisted of Skylab, \n",
    "#                          a space station that supported three manned missions in 1973-74, \n",
    "#                          and the Apollo-Soyuz Test Project, a joint Earth orbit mission with \n",
    "#                          the Soviet Union in 1975.\"\"\",\n",
    "#                     \"qas\":[{\n",
    "#                         \"id\": 'Q1', \n",
    "#                         \"question\":\"What project put the first Americans into space?\"}]}]}]\n",
    "\n",
    "vocab_file = 'vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "request_body, inputs_dict, eval_examples, eval_features = get_bert_request_body(input_data, \n",
    "                                     version_2_with_negative, \n",
    "                                     tokenizer, \n",
    "                                     max_seq_length, \n",
    "                                     doc_stride,\n",
    "                                     max_query_length,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://alpha-ml.googleapis.com/v1/projects/k80-exploration/models/bert/versions/dongm01:predict\"\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'Authorization': 'Bearer {}'.format(\n",
    "      os.popen('gcloud auth application-default print-access-token').read().rstrip())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.request(\"POST\", url, \n",
    "                 headers=headers, \n",
    "                 data = request_body).content\n",
    "response_data = ast.literal_eval(response.decode(\"UTF-8\"))\n",
    "start_logits = response_data[\"outputs\"][0]['data'][0::2]\n",
    "end_logits = response_data[\"outputs\"][0]['data'][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What project put the first Americans into space?\n",
      "Answer: Project Mercury\n"
     ]
    }
   ],
   "source": [
    "from run_squad import get_predictions, RawResult\n",
    "\n",
    "all_results = []\n",
    "unique_id = inputs_dict['unique_ids'][0][0]\n",
    "\n",
    "all_results.append(\n",
    "    RawResult(\n",
    "        start_logits=start_logits,\n",
    "        end_logits=end_logits,\n",
    "        unique_id=unique_id)\n",
    ")\n",
    "\n",
    "all_predictions, all_nbest_json, scores_diff_json = get_predictions(\n",
    "          eval_examples, eval_features, all_results,\n",
    "          n_best_size, max_answer_length,\n",
    "          do_lower_case, version_2_with_negative,\n",
    "          verbose_logging)\n",
    "\n",
    "qas_id = input_data[0]['paragraphs'][0]['qas'][0]['id']\n",
    "question = input_data[0]['paragraphs'][0]['qas'][0]['question']\n",
    "\n",
    "print(f'Question: {question}')\n",
    "print(f'Answer: {all_predictions[qas_id]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
