{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {},
   "source": [
    "# Serving Models with GCP Vertex AI Prediction and NVIDIA Triton Server\n",
    "\n",
    "This notebook demonstrates how to serve example models using NVIDIA Triton Inference Server and Vertex AI Prediction.\n",
    "The notebook compiles prescriptive guidance for the following tasks:\n",
    "\n",
    "1. Download Triton sample models\n",
    "2. Registering and deploying the models with NGC Triton images into Vertex Prediction Models and Endpoints.\n",
    "3. Getting online predictions from the deployed models.\n",
    "\n",
    "To fully benefit from the content covered in this notebook, you should have a solid understanding of key Vertex AI Prediction concepts like models, endpoints, and model deployments. We strongly recommend reviewing [Vertex AI Prediction documentation](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) before proceeding.\n",
    "\n",
    "### Triton Inference Server Overview\n",
    "\n",
    "[Triton Inference Server](https://github.com/triton-inference-server/server) provides an inference solution optimized for both CPUs and GPUs. Triton can run multiple models from the same or different frameworks concurrently on a single GPU or CPU. In a multi-GPU server, it automatically creates an instance of each model on each GPU to increase utilization without extra coding. It supports real-time inferencing, batch inferencing to maximize GPU/CPU utilization, and streaming inference with built-in support for audio streaming input. It also supports model ensembles for use cases that require multiple models to perform end-to-end inference.\n",
    "\n",
    "The following figure shows the Triton Inference Server high-level architecture.\n",
    "\n",
    "<img src=\"triton_nvidia.png\" style=\"width:70%\"/>\n",
    "\n",
    "- The model repository is a file-system based repository of the models that Triton will make available for inference.\n",
    "- Inference requests arrive at the server via either HTTP/REST or gRPC and are then routed to the appropriate per-model scheduler. \n",
    "- Triton implements multiple scheduling and batching algorithms that can be configured on a model-by-model basis.\n",
    "- The backend performs inference using the inputs provided in the batched requests to produce the requested outputs.\n",
    "\n",
    "Triton server provides readiness and liveness health endpoints, as well as utilization, throughput, and latency metrics, which enable the integration of Triton into deployment environments, such as Vertex AI Prediction.\n",
    "\n",
    "Refer to [Triton Inference Server Architecture](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md) for more detailed information.\n",
    "\n",
    "### Triton Inference Server on Vertex AI Prediction\n",
    "\n",
    "In this section, we describe the deployment of Triton Inference Server on Vertex AI Prediction.\n",
    "\n",
    "Triton Inference Server runs inside a container published by NVIDIA GPU Cloud - [NVIDIA Triton Inference Server Image](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver). NVIDIA and GCP Vertex team collabrate and adds packages and configurations to align Triton with Vertex AI [requirements for custom serving container images](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements).\n",
    "\n",
    "An ML model to be served by Triton is registered with Vertex AI Prediction as a `Model`. The `Model`'s metadata reference a location of the ensemble artifacts in Google Cloud Storage and the custom serving container and its configurations. \n",
    "\n",
    "Triton loads the models and exposes inference, health, and model management REST endpoints using [standard inference protocols](https://github.com/kserve/kserve/tree/master/docs/predict-api/v2). While deploying to Vertex, Triton will recognize Vertex environment and adopt Vertex AI Prediction protocol for [health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health) and predictions.\n",
    "\n",
    "To invoke the model through the Vertex AI Prediction endpoint you need to format your request using a [standard Inference Request JSON Object](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference) or a [Inference Request JSON Object with a binary extension](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md) and submit a request to Vertex AI Prediction [REST rawPredict endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/rawPredict). You need to use the `rawPredict` rather than `predict` endpoint because inference request formats used by Triton are not compatible with the Vertex AI Prediction [standard input format](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#formatting-prediction-input).\n",
    "\n",
    "\n",
    "### Notebook flow\n",
    "\n",
    "This notebook assumes that you have access to GCP project and Vertex AI Prediction, it is highly recommended to register with NVIDIA GPU Cloud.\n",
    "\n",
    "As you walk through the notebook you will execute the following tasks:\n",
    "\n",
    "- Configure the notebook environment settings, including GCP project, compute region, and the GCS locations of a sample model\n",
    "- Register the model as a Vertex Prediction model with Triton container image\n",
    "- Create a Vertex Prediction endpoint\n",
    "- Deploy the model endpoint\n",
    "- Invoke the deployed ensemble model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b3137-249c-49a3-a8be-f459b3774ea2",
   "metadata": {},
   "source": [
    "## 1, Setup\n",
    "\n",
    "In this section of the notebook you configure your environment settings, including a GCP project, a Vertex AI compute region, and a Vertex AI staging GCS bucket. \n",
    "You also set the locations of downloaded sample model and a set of constants that are used to create names and display names of Vertex AI Prediction resources.\n",
    "\n",
    "First, fetch the model from [Triton Samples Model Repository](https://github.com/triton-inference-server/server/blob/main/docs/examples/fetch_models.sh), and copy the simple model into a GCS bucket - `gs://triton_model_repository/models/`.\n",
    "\n",
    "Second, push a Triton container image into GCR (`nvcr.io/nvidia/tritonserver:21.12-py3` -> `gcr.io/{PROJECT_ID}/{IMAGE_NAME}`)\n",
    "\n",
    "Make sure to update the below cells with the values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791d621-22e8-41bc-8681-a48aec45ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d16e2-f098-4ab0-bed9-41665eec3790",
   "metadata": {},
   "source": [
    "Set the below constants to your project id, a compute region for Vertex AI and a GCS bucket that will be used for Vertex AI staging and storing exported model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e20da19-0dda-4a58-b606-341e50c9bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'merlin-on-gcp' # Change to your project.\n",
    "REGION = 'us-central1'  # Change to your region.\n",
    "STAGING_BUCKET = 'triton_model_repository' # Change to your bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8acbb-305a-412d-904b-b9d274852e53",
   "metadata": {},
   "source": [
    "`MODEL_ARTIFACTS_REPOSITORY` is a root GCS location where the Triton model artifacts will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b732f8-6837-4c82-99a3-9b09a1298898",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ARTIFACTS_REPOSITORY = f'gs://{STAGING_BUCKET}/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f3935-c192-4e77-bdf9-687c9bff79bc",
   "metadata": {},
   "source": [
    "The following set of constants will be used to create names and display names of Vertex Prediction resources like models, endpoints, and model deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d994ba-d39c-4b3d-a3ae-776f9eb995cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'simple'\n",
    "MODEL_VERSION = 'v00'\n",
    "MODEL_DISPLAY_NAME = f'triton-{MODEL_NAME}-{MODEL_VERSION}'\n",
    "ENDPOINT_DISPLAY_NAME = f'endpoint-{MODEL_NAME}-{MODEL_VERSION}'\n",
    "\n",
    "IMAGE_NAME = 'triton-deploy'\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e3f45-2bab-48fb-9b81-4e0e65635ece",
   "metadata": {},
   "source": [
    "## 2. Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0cca3-11e6-4454-af41-c5c632dbbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f7ae3-76c4-4bfe-8a80-44d3567b0b6d",
   "metadata": {},
   "source": [
    "## 3. Uploading the model and its metadata to Vertex Models.\n",
    "\n",
    "Refer to [Use a custom container for prediction guide](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container) for detailed information about creating Vertex AI Prediction `Model` resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3c2ac-90fb-4175-b185-d2981cbdebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_container_command = ["tritonserver"]\n",
    "\n",
    "model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    serving_container_image_uri=IMAGE_URI,\n",
    "    serving_container_command=serving_container_command,\n",
    "    artifact_uri=MODEL_ARTIFACTS_REPOSITORY,\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c572a7c-c5e5-4383-bbd5-ce11b6659945",
   "metadata": {},
   "source": [
    "## 4. Deploying the model to Vertex AI Prediction.\n",
    "\n",
    "Deploying a Vertex AI Prediction `Model` is a two step process. First you create an endpoint that will expose an external interface to clients consuming the model. After the endpoint is ready you can deploy multiple versions of a model to the endpoint.\n",
    "\n",
    "Refer to [Deploy a model using the Vertex AI API guide](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) for more information about the APIs used in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763abf60-c325-4222-b550-0aca98492d15",
   "metadata": {},
   "source": [
    "### Create the Vertex Endpoint\n",
    "\n",
    "Before deploying the model you need to create a Vertex AI Prediction endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef58a0-1f13-40a7-b545-f6a75ee0fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertex_ai.Endpoint.create(\n",
    "    display_name=ENDPOINT_DISPLAY_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fd6fc-2777-457e-b446-8b1532984746",
   "metadata": {},
   "source": [
    "### Deploy the model to Vertex Prediction endpoint\n",
    "\n",
    "After the endpoint is ready, you can deploy your model to the endpoint. You will run the Triton Server on a GPU node equipped with the NVIDIA Tesla T4 GPUs.\n",
    "\n",
    "Refer to [Deploy a model using the Vertex AI API guide](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bb85f-330c-4ce6-9a8f-d5b96772ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-4\"\n",
    "accelerator_type=\"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "min_replica_count = 1\n",
    "max_replica_count = 2\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=MODEL_DISPLAY_NAME,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "endpoint.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c71d4-56c4-4f24-aee6-d1b96f6e4e8a",
   "metadata": {},
   "source": [
    "## 5. Invoking the model\n",
    "\n",
    "To invoke the model through Vertex AI Prediction endpoint you need to format your request using a [standard Inference Request JSON Object](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference) or a [Inference Request JSON Object with a binary extension](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md) and submit a request to Vertex AI Prediction [REST rawPredict endpoint](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/rawPredict). You need to use the `rawPredict` rather than `predict` endpoint because inference request formats used by Triton are not compatible with the Vertex AI Prediction [standard input format](https://cloud.google.com/vertex-ai/docs/predictions/online-predictions-custom-models#formatting-prediction-input).\n",
    "\n",
    "The below cell shows a sample request body formatted as a [standard Inference Request JSON Object](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924669cf-5442-4454-b881-e2a792a96f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "payload = {\n",
    "    \"id\": \"0\",\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"INPUT0\",\n",
    "            \"shape\": [\n",
    "                1,\n",
    "                16\n",
    "            ],\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"parameters\": {},\n",
    "            \"data\": [\n",
    "                0,\n",
    "                1,\n",
    "                2,\n",
    "                3,\n",
    "                4,\n",
    "                5,\n",
    "                6,\n",
    "                7,\n",
    "                8,\n",
    "                9,\n",
    "                10,\n",
    "                11,\n",
    "                12,\n",
    "                13,\n",
    "                14,\n",
    "                15\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"INPUT1\",\n",
    "            \"shape\": [\n",
    "                1,\n",
    "                16\n",
    "            ],\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"parameters\": {},\n",
    "            \"data\": [\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1,\n",
    "                -1\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('simple.json', 'w') as f:\n",
    "    json.dump(payload, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27197c74-94c7-454f-a469-151c680de97b",
   "metadata": {},
   "source": [
    "You can invoke the Vertex AI Prediction `rawPredict` endpoint using any HTTP tool or library, including `curl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f98d37f-87bf-47d8-951e-a88dee48421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"0\",\"model_name\":\"simple\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"OUTPUT0\",\"datatype\":\"INT32\",\"shape\":[1,16],\"data\":[-1,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]},{\"name\":\"OUTPUT1\",\"datatype\":\"INT32\",\"shape\":[1,16],\"data\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]}]}"
     ]
    }
   ],
   "source": [
    "uri = f'https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint.name}:rawPredict'\n",
    "\n",
    "! curl -X POST \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\"  \\\n",
    "{uri} \\\n",
    "-d @simple.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f1b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "",
   "name": "common-cpu.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
