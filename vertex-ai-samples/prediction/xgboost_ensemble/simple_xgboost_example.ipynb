{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton server with FIL backend in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook shows the procedure to deploy an ensemble of [XGBoost model](https://xgboost.readthedocs.io/en/latest/) in Triton Inference Server with Forest Inference Library (FIL) backend. The FIL backend allows forest models trained by several popular machine learning frameworks (including XGBoost, LightGBM, Scikit-Learn, and cuML) to be deployed in a Triton inference server using the RAPIDS Forest Inference LIbrary for fast GPU-based inference. Using this backend, forest models can be deployed seamlessly alongside deep learning models for fast, unified inference pipelines.\n",
    "\n",
    "For the ensemble, [Business Logic Scripting(BLS)](https://github.com/triton-inference-server/python_backend/blob/main/README.md#business-logic-scripting) in Triton python custom backend is used.\n",
    "\n",
    "This notebooks runs in Triton NGC docker container: nvcr.io/nvidia/tritonserver:22.01-py3\n",
    "\n",
    "### Contents\n",
    "* [Train XGBoost model on dummy data](http://localhost:7001/notebooks/simple_xgboost_example.ipynb#Train-XGBoost-model)\n",
    "* [Export, load and deploy XGBoost model in Triton Inference Server](http://localhost:8888/notebooks/simple_xgboost_example.ipynb#Export,-load-and-deploy-XGBoost-model-in-Triton-Inference-Server)\n",
    "* [Determine throughput and latency using Perf Analyzer](http://localhost:7001/notebooks/simple_xgboost_example.ipynb#Determine-throughput-and-latency-with-Perf-Analyzer)\n",
    "* [Find best configuration using Model Analyzer](http://localhost:7001/notebooks/simple_xgboost_example.ipynb#Find-best-configuration-using-Model-Analyzer)\n",
    "* [Deploy model with best configuration](http://localhost:7001/notebooks/simple_xgboost_example.ipynb#Deploy-model-with-best-configuration)\n",
    "* [Triton Client](http://localhost:7001/notebooks/simple_xgboost_example.ipynb#Triton-Client)\n",
    "* [Conclusion](http://localhost:7001/notebooks/simple_xgboost_example.ipynb#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* Nvidia GPU (Pascal+ Recommended GPUs: T4, V100 or A100)\n",
    "* [Latest NVIDIA driver](https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html)\n",
    "* [Docker](https://docs.docker.com/get-docker/)\n",
    "* [The NVIDIA container toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To begin, check that the NVIDIA driver has been installed correctly. The `nvidia-smi` command should run and output information about the GPUs on your system:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install XGBoost and Sklearn\n",
    "\n",
    "We'd need to install XGBoost and SKlearn using the following pip3 commands inside the container as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sklearn and XGBoost\n",
    "!pip3 install -U scikit-learn xgboost cupy-cuda115 google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost model\n",
    "\n",
    "If you have a pre-trained xgboost model, save it as `xgboost.model` and skip this step. We'll train a XGBoost model on random data in this section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data to perform binary classification\n",
    "seed = 7\n",
    "features = 9 # number of sample features\n",
    "samples = 10000 # number of samples\n",
    "X = numpy.random.rand(samples, features).astype('float32')\n",
    "Y = numpy.random.randint(2, size=samples)\n",
    "\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.2f}\".format(accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export, load and deploy XGBoost model in Triton Inference Server\n",
    "\n",
    "For deploying the trained XGBoost model in Triton Inference Server, follow the steps below:\n",
    "\n",
    "**1. Create a model repository and save xgboost model checkpoint:**\n",
    "\n",
    "We'll need to create a model repository that looks as follows, and create 11 of them for the ensemble model:\n",
    "\n",
    "```\n",
    "model_repository/\n",
    "`-- fil_0\n",
    "    |-- 1\n",
    "    |   `-- xgboost.model\n",
    "    `-- config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save the model\n",
    "!for i in {0..10}; do mkdir -p ./model_repository/fil_\"$i\"/1; done\n",
    "\n",
    "# Save your xgboost model as xgboost.model\n",
    "# For more information on saving xgboost model check https://xgboost.readthedocs.io/en/latest/python/python_intro.html#training\n",
    "# Model can also be dumped to json format\n",
    "for i in range(0,11):\n",
    "    model.save_model('./model_repository/fil_{}/1/xgboost.model'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "The FIL backend's testing infrastructure includes a script for generating example models, putting them in the correct directory layout, and generating an associated config file. This can be helpful both for providing a template for your own models and for testing your Triton deployment. Please check this [link](https://github.com/triton-inference-server/fil_backend/blob/main/Example_Models.md) for the sample script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create and save config.pbtxt**\n",
    "\n",
    "To deploy the model in Triton Inference Server, we need to create and save a protobuf config file called config.pbtxt under `model_repository/fil_0/` directory that contains information about the model and the deployment. Sample config file is available here: [link](https://github.com/triton-inference-server/fil_backend#configuration)\n",
    "\n",
    "Essentially, the following parameters need to be updated as per your configuration\n",
    "\n",
    "```\n",
    "name: \"fil_0\"                              # Name of the model directory (fil in our case)\n",
    "backend: \"fil\"                           # Triton FIL backend for deploying forest models\n",
    "max_batch_size: 8192\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 9 ]                          # Input feature dimensions, in our sample case it's 9\n",
    "  }\n",
    "]\n",
    "output [\n",
    " {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]                          # Output 2 for binary classification model\n",
    "  }\n",
    "]\n",
    "instance_group [{ kind: KIND_GPU }]\n",
    "parameters [\n",
    "  {\n",
    "    key: \"model_type\"\n",
    "    value: { string_value: \"xgboost\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"predict_proba\"\n",
    "    value: { string_value: \"false\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"output_class\"\n",
    "    value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"threshold\"\n",
    "    value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"algo\"\n",
    "    value: { string_value: \"ALGO_AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"storage_type\"\n",
    "    value: { string_value: \"AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"blocks_per_sm\"\n",
    "    value: { string_value: \"0\" }\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Triton server looks for this configuration file before deploying XGBoost model for inference. It'll setup the server parameters as per the configuration passed within config.pbtxt. Store the above config at `/model_repository/fil_0/` directory as config.pbtxt as follows:\n",
    "\n",
    "For more information on sample configs, please refer this [link](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Writing config to file\n",
    "for i in {0..10}\n",
    "do cat > ./model_repository/fil_$i/config.pbtxt <<EOL \n",
    "name: \"fil_$i\"                              # Name of the model directory (fil in our case)\n",
    "backend: \"fil\"                           # Triton FIL backend for deploying forest models\n",
    "max_batch_size: 8192\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 9 ]                          # Input feature dimensions, in our sample case it's 9\n",
    "  }\n",
    "]\n",
    "output [\n",
    " {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]                          # Output 2 for binary classification model\n",
    "  }\n",
    "]\n",
    "instance_group [{ kind: KIND_GPU }]\n",
    "parameters [\n",
    "  {\n",
    "    key: \"model_type\"\n",
    "    value: { string_value: \"xgboost\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"predict_proba\"\n",
    "    value: { string_value: \"false\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"output_class\"\n",
    "    value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"threshold\"\n",
    "    value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"algo\"\n",
    "    value: { string_value: \"ALGO_AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"storage_type\"\n",
    "    value: { string_value: \"AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"blocks_per_sm\"\n",
    "    value: { string_value: \"0\" }\n",
    "  }\n",
    "]\n",
    "\n",
    "EOL\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model repository should look like this, and has 11 folders in total, the 11 xgboost models will be ensembled by bls_async model:\n",
    "\n",
    "```\n",
    "model_repository/\n",
    "`-- fil_0\n",
    "    |-- 1\n",
    "    |   `-- xgboost.model\n",
    "    `-- config.pbtxt\n",
    "```\n",
    "\n",
    "more about triton BLS ensemble model can be learned [here](https://github.com/triton-inference-server/python_backend/blob/main/examples/bls/README.md#asynchronous-bls-requests)\n",
    "\n",
    "**3. Deploy the model in Triton Inference Server**\n",
    "\n",
    "Finally, we can deploy the xgboost model in Triton Inference Server using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0216 20:28:28.824433 9336 metrics.cc:298] Collecting metrics for GPU 0: Tesla T4\n",
      "I0216 20:28:29.245799 9336 libtorch.cc:1227] TRITONBACKEND_Initialize: pytorch\n",
      "I0216 20:28:29.245821 9336 libtorch.cc:1237] Triton TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.245842 9336 libtorch.cc:1243] 'pytorch' TRITONBACKEND API version: 1.7\n",
      "2022-02-16 20:28:29.410012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-02-16 20:28:29.447749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "I0216 20:28:29.447820 9336 tensorflow.cc:2176] TRITONBACKEND_Initialize: tensorflow\n",
      "I0216 20:28:29.447839 9336 tensorflow.cc:2186] Triton TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.447844 9336 tensorflow.cc:2192] 'tensorflow' TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.447848 9336 tensorflow.cc:2216] backend configuration:\n",
      "{}\n",
      "I0216 20:28:29.453320 9336 onnxruntime.cc:2232] TRITONBACKEND_Initialize: onnxruntime\n",
      "I0216 20:28:29.453339 9336 onnxruntime.cc:2242] Triton TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.453344 9336 onnxruntime.cc:2248] 'onnxruntime' TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.453347 9336 onnxruntime.cc:2278] backend configuration:\n",
      "{}\n",
      "I0216 20:28:29.484279 9336 openvino.cc:1234] TRITONBACKEND_Initialize: openvino\n",
      "I0216 20:28:29.484293 9336 openvino.cc:1244] Triton TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.484313 9336 openvino.cc:1250] 'openvino' TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.776622 9336 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f7cdc000000' with size 268435456\n",
      "I0216 20:28:29.778737 9336 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0216 20:28:29.809026 9336 model_repository_manager.cc:994] loading: fil_1:1\n",
      "I0216 20:28:29.909414 9336 model_repository_manager.cc:994] loading: fil_0:1\n",
      "I0216 20:28:29.920276 9336 initialize.hpp:43] TRITONBACKEND_Initialize: fil\n",
      "I0216 20:28:29.920316 9336 backend.hpp:47] Triton TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.920336 9336 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.7\n",
      "I0216 20:28:29.922867 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_1 (version 1)\n",
      "I0216 20:28:29.931891 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_1_0 (GPU device 0)\n",
      "I0216 20:28:30.009749 9336 model_repository_manager.cc:994] loading: bls_async:1\n",
      "I0216 20:28:30.110251 9336 model_repository_manager.cc:994] loading: fil_10:1\n",
      "I0216 20:28:30.210740 9336 model_repository_manager.cc:994] loading: fil_2:1\n",
      "I0216 20:28:30.311223 9336 model_repository_manager.cc:994] loading: fil_3:1\n",
      "I0216 20:28:30.411747 9336 model_repository_manager.cc:994] loading: fil_4:1\n",
      "I0216 20:28:30.512254 9336 model_repository_manager.cc:994] loading: fil_5:1\n",
      "I0216 20:28:30.612860 9336 model_repository_manager.cc:994] loading: fil_6:1\n",
      "I0216 20:28:30.713385 9336 model_repository_manager.cc:994] loading: fil_7:1\n",
      "I0216 20:28:30.813901 9336 model_repository_manager.cc:994] loading: fil_8:1\n",
      "I0216 20:28:30.914390 9336 model_repository_manager.cc:994] loading: fil_9:1\n",
      "I0216 20:28:32.400002 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_0 (version 1)\n",
      "I0216 20:28:32.400201 9336 model_repository_manager.cc:1149] successfully loaded 'fil_1' version 1\n",
      "I0216 20:28:32.414249 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_10 (version 1)\n",
      "I0216 20:28:32.415934 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_2 (version 1)\n",
      "I0216 20:28:32.417579 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_3 (version 1)\n",
      "I0216 20:28:32.419214 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_4 (version 1)\n",
      "I0216 20:28:32.420898 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_5 (version 1)\n",
      "I0216 20:28:32.422760 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_6 (version 1)\n",
      "I0216 20:28:32.424620 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_7 (version 1)\n",
      "I0216 20:28:32.426251 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_0_0 (GPU device 0)\n",
      "I0216 20:28:32.472886 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_8 (version 1)\n",
      "I0216 20:28:32.474289 9336 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil_9 (version 1)\n",
      "I0216 20:28:32.474497 9336 model_repository_manager.cc:1149] successfully loaded 'fil_0' version 1\n",
      "I0216 20:28:32.476896 9336 python.cc:1897] TRITONBACKEND_ModelInstanceInitialize: bls_async_0 (GPU device 0)\n",
      "I0216 20:28:33.354830 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_10_0 (GPU device 0)\n",
      "I0216 20:28:33.355183 9336 model_repository_manager.cc:1149] successfully loaded 'bls_async' version 1\n",
      "I0216 20:28:33.406259 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_8_0 (GPU device 0)\n",
      "I0216 20:28:33.407524 9336 model_repository_manager.cc:1149] successfully loaded 'fil_10' version 1\n",
      "I0216 20:28:33.434862 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_3_0 (GPU device 0)\n",
      "I0216 20:28:33.434993 9336 model_repository_manager.cc:1149] successfully loaded 'fil_8' version 1\n",
      "I0216 20:28:33.456345 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_4_0 (GPU device 0)\n",
      "I0216 20:28:33.457803 9336 model_repository_manager.cc:1149] successfully loaded 'fil_3' version 1\n",
      "I0216 20:28:33.468681 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_5_0 (GPU device 0)\n",
      "I0216 20:28:33.470567 9336 model_repository_manager.cc:1149] successfully loaded 'fil_4' version 1\n",
      "I0216 20:28:33.486283 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_6_0 (GPU device 0)\n",
      "I0216 20:28:33.487628 9336 model_repository_manager.cc:1149] successfully loaded 'fil_5' version 1\n",
      "I0216 20:28:33.500616 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_7_0 (GPU device 0)\n",
      "I0216 20:28:33.501975 9336 model_repository_manager.cc:1149] successfully loaded 'fil_6' version 1\n",
      "I0216 20:28:33.523967 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_2_0 (GPU device 0)\n",
      "I0216 20:28:33.525332 9336 model_repository_manager.cc:1149] successfully loaded 'fil_7' version 1\n",
      "I0216 20:28:33.537223 9336 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_9_0 (GPU device 0)\n",
      "I0216 20:28:33.539635 9336 model_repository_manager.cc:1149] successfully loaded 'fil_2' version 1\n",
      "I0216 20:28:33.564501 9336 model_repository_manager.cc:1149] successfully loaded 'fil_9' version 1\n",
      "I0216 20:28:33.564669 9336 server.cc:519] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0216 20:28:33.564802 9336 server.cc:546] \n",
      "+-------------+-------------------------------------------------------------------------+--------+\n",
      "| Backend     | Path                                                                    | Config |\n",
      "+-------------+-------------------------------------------------------------------------+--------+\n",
      "| pytorch     | /opt/tritonserver/backends/pytorch/libtriton_pytorch.so                 | {}     |\n",
      "| onnxruntime | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so         | {}     |\n",
      "| openvino    | /opt/tritonserver/backends/openvino_2021_2/libtriton_openvino_2021_2.so | {}     |\n",
      "| fil         | /opt/tritonserver/backends/fil/libtriton_fil.so                         | {}     |\n",
      "| tensorflow  | /opt/tritonserver/backends/tensorflow1/libtriton_tensorflow1.so         | {}     |\n",
      "| python      | /opt/tritonserver/backends/python/libtriton_python.so                   | {}     |\n",
      "+-------------+-------------------------------------------------------------------------+--------+\n",
      "\n",
      "I0216 20:28:33.564943 9336 server.cc:589] \n",
      "+-----------+---------+--------+\n",
      "| Model     | Version | Status |\n",
      "+-----------+---------+--------+\n",
      "| bls_async | 1       | READY  |\n",
      "| fil_0     | 1       | READY  |\n",
      "| fil_1     | 1       | READY  |\n",
      "| fil_10    | 1       | READY  |\n",
      "| fil_2     | 1       | READY  |\n",
      "| fil_3     | 1       | READY  |\n",
      "| fil_4     | 1       | READY  |\n",
      "| fil_5     | 1       | READY  |\n",
      "| fil_6     | 1       | READY  |\n",
      "| fil_7     | 1       | READY  |\n",
      "| fil_8     | 1       | READY  |\n",
      "| fil_9     | 1       | READY  |\n",
      "+-----------+---------+--------+\n",
      "\n",
      "I0216 20:28:33.565150 9336 tritonserver.cc:1865] \n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Option                           | Value                                                                                                                                                                                  |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| server_id                        | triton                                                                                                                                                                                 |\n",
      "| server_version                   | 2.18.0                                                                                                                                                                                 |\n",
      "| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |\n",
      "| model_repository_path[0]         | ./model_repository                                                                                                                                                                     |\n",
      "| model_control_mode               | MODE_NONE                                                                                                                                                                              |\n",
      "| strict_model_config              | 1                                                                                                                                                                                      |\n",
      "| rate_limit                       | OFF                                                                                                                                                                                    |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                               |\n",
      "| response_cache_byte_size         | 0                                                                                                                                                                                      |\n",
      "| min_supported_compute_capability | 6.0                                                                                                                                                                                    |\n",
      "| strict_readiness                 | 1                                                                                                                                                                                      |\n",
      "| exit_timeout                     | 30                                                                                                                                                                                     |\n",
      "+----------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "I0216 20:28:33.570764 9336 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0216 20:28:33.571110 9336 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000\n",
      "I0216 20:28:33.613123 9336 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    }
   ],
   "source": [
    "# Run the Triton Inference Server in a Subprocess from Jupyter notebook\n",
    "\n",
    "triton_process = subprocess.Popen([\"tritonserver\", \"--model-repository=./model_repository\"], stdout=subprocess.PIPE, preexec_fn=os.setsid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command should load the model and print the log `successfully loaded 'fil' version 1`. Triton server listens on the following endpoints:\n",
    "\n",
    "```\n",
    "Port 8000    -> HTTP Service\n",
    "Port 8001    -> GRPC Service\n",
    "Port 8002    -> Metrics\n",
    "```\n",
    "\n",
    "We can test the status of the server connection by running the curl command: `curl -v <IP of machine>:8000/v2/health/ready` which should return `HTTP/1.1 200 OK`\n",
    "\n",
    "**NOTE:-** In our case the IP of machine on which Triton Server and this notebook are currently running is `localhost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\r\n",
      "* TCP_NODELAY set\r\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\r\n",
      "> GET /v2/health/ready HTTP/1.1\r",
      "\r\n",
      "> Host: localhost:8000\r",
      "\r\n",
      "> User-Agent: curl/7.68.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> \r",
      "\r\n",
      "* Mark bundle as not supporting multiuse\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Content-Length: 0\r",
      "\r\n",
      "< Content-Type: text/plain\r",
      "\r\n",
      "< \r",
      "\r\n",
      "* Connection #0 to host localhost left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8416 sha256=0042995cf2b16f039b45625917ae738d72bf6c817a7bfb981be66c8658900498\n",
      "  Stored in directory: /root/.cache/pip/wheels/e0/c2/fb/5cf4e1cfaf28007238362cb746fb38fc2dd76348331a748d54\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install nvidia-pyindex\n",
    "!pip3 install nvidia-pyindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting tritonclient[http]\n",
      "  Downloading tritonclient-2.18.0-py3-none-manylinux1_x86_64.whl (7.8 MB)\n",
      "     |████████████████████████████████| 7.8 MB 6.4 MB/s            \n",
      "\u001b[?25hCollecting python-rapidjson>=0.9.1\n",
      "  Downloading python_rapidjson-1.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.5 MB)\n",
      "     |████████████████████████████████| 1.5 MB 32.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.8/dist-packages (from tritonclient[http]) (1.22.1)\n",
      "Collecting geventhttpclient>=1.4.4\n",
      "  Downloading geventhttpclient-1.5.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (77 kB)\n",
      "     |████████████████████████████████| 77 kB 32.2 MB/s            \n",
      "\u001b[?25hCollecting gevent>=0.13\n",
      "  Downloading gevent-21.12.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "     |████████████████████████████████| 6.5 MB 30.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (2019.11.28)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "     |████████████████████████████████| 357 kB 37.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (1.14.0)\n",
      "Collecting zope.event\n",
      "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (60.5.0)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.4.0-cp38-cp38-manylinux2010_x86_64.whl (259 kB)\n",
      "     |████████████████████████████████| 259 kB 17.2 MB/s            \n",
      "\u001b[?25hCollecting greenlet<2.0,>=1.1.0\n",
      "  Downloading greenlet-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "     |████████████████████████████████| 156 kB 33.8 MB/s            \n",
      "\u001b[?25hInstalling collected packages: zope.interface, zope.event, greenlet, python-rapidjson, gevent, brotli, tritonclient, geventhttpclient\n",
      "Successfully installed brotli-1.0.9 gevent-21.12.0 geventhttpclient-1.5.3 greenlet-1.1.2 python-rapidjson-1.5 tritonclient-2.18.0 zope.event-4.5.0 zope.interface-5.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Triton client\n",
    "!pip3 install tritonclient[http]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Client\n",
    "\n",
    "After model profiling is done and the final model is selected as per required configuration and deployed in Triton, we can now test the inference by sending real inference request from Triton Client and checking the accuracy of responses. For more information on installation steps, please check [Triton Client Github](https://github.com/triton-inference-server/client)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check client library can be imported\n",
    "import numpy\n",
    "import tritonclient.http as triton_http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 9 # number of sample features\n",
    "samples = 8192 # number of samples\n",
    "X = numpy.random.rand(samples, features).astype('float32')\n",
    "with open('dtest.npy', 'wb') as f:\n",
    "    numpy.save(f, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78.5 ms, sys: 41.2 ms, total: 120 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tritonclient.utils import *\n",
    "import sys\n",
    "import tritonclient.grpc as triton_grpc\n",
    "\n",
    "import numpy as np\n",
    "with open('dtest.npy', 'rb') as f:\n",
    "    dtest = np.load(f)\n",
    "\n",
    "model_name = \"bls_async\"\n",
    "\n",
    "with triton_grpc.InferenceServerClient(\"localhost:8001\") as client:\n",
    "    for n in range(1,100):\n",
    "        triton_input = triton_grpc.InferInput('input__0',\n",
    "                                              (dtest.shape[0], dtest.shape[1]),\n",
    "                                              'FP32'\n",
    "                                             )\n",
    "\n",
    "        triton_input.set_data_from_numpy(dtest)\n",
    "\n",
    "        triton_output = triton_grpc.InferRequestedOutput('output__0')\n",
    "\n",
    "        response = client.infer(model_name,\n",
    "                                inputs=[triton_input],\n",
    "                                request_id=str(1),\n",
    "                                outputs=[triton_output]\n",
    "                               )\n",
    "\n",
    "        result = response.get_response()\n",
    "\n",
    "    output0_data = response.as_numpy(\"output__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User can follow [Triton Vertex notebook](https://github.com/NVIDIA/nvidia-gcp-samples/blob/master/vertex-ai-samples/prediction/triton_inference.ipynb) to deploy above model to GCP Vertex AI Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0216 20:28:19.448369 6686 server.cc:249] Waiting for in-flight requests to complete.\n",
      "I0216 20:28:19.448418 6686 model_repository_manager.cc:1026] unloading: fil_9:1\n",
      "I0216 20:28:19.448620 6686 model_repository_manager.cc:1026] unloading: fil_7:1\n",
      "I0216 20:28:19.448806 6686 model_repository_manager.cc:1026] unloading: fil_6:1\n",
      "I0216 20:28:19.448979 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.449061 6686 model_repository_manager.cc:1026] unloading: fil_5:1\n",
      "I0216 20:28:19.449156 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.449304 6686 model_repository_manager.cc:1026] unloading: fil_4:1\n",
      "I0216 20:28:19.449408 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.449465 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.449474 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.449547 6686 model_repository_manager.cc:1026] unloading: fil_3:1\n",
      "I0216 20:28:19.449571 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_7' version 1I0216 20:28:19.449572 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_9' version 1\n",
      "\n",
      "I0216 20:28:19.449648 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.449669 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.449742 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_6' version 1\n",
      "I0216 20:28:19.449947 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.450004 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_5' version 1\n",
      "I0216 20:28:19.450037 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.450104 6686 model_repository_manager.cc:1026] unloading: fil_8:1\n",
      "I0216 20:28:19.450190 6686 model_repository_manager.cc:1026] unloading: fil_2:1\n",
      "I0216 20:28:19.450307 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.450365 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_4' version 1\n",
      "I0216 20:28:19.450368 6686 model_repository_manager.cc:1026] unloading: fil_1:1\n",
      "I0216 20:28:19.450405 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.450609 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.450670 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.450731 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_3' version 1\n",
      "I0216 20:28:19.450794 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.450848 6686 model_repository_manager.cc:1026] unloading: fil_0:1\n",
      "I0216 20:28:19.450865 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.450918 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_8' version 1\n",
      "I0216 20:28:19.450928 6686 model_repository_manager.cc:1026] unloading: fil_10:1\n",
      "I0216 20:28:19.451037 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.451087 6686 model_repository_manager.cc:1026] unloading: bls_async:1\n",
      "I0216 20:28:19.451103 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_2' version 1\n",
      "I0216 20:28:19.451164 6686 server.cc:264] Timeout 30: Found 4 live models and 0 in-flight non-inference requests\n",
      "I0216 20:28:19.451207 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.451563 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.451763 6686 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0216 20:28:19.452166 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.452247 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_10' version 1\n",
      "I0216 20:28:19.452409 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.452506 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_0' version 1\n",
      "I0216 20:28:19.453339 6686 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0216 20:28:19.453442 6686 model_repository_manager.cc:1132] successfully unloaded 'fil_1' version 1\n",
      "I0216 20:28:20.451310 6686 server.cc:264] Timeout 29: Found 1 live models and 0 in-flight non-inference requests\n",
      "I0216 20:28:20.761933 6686 model_repository_manager.cc:1132] successfully unloaded 'bls_async' version 1\n",
      "I0216 20:28:21.451513 6686 server.cc:264] Timeout 28: Found 0 live models and 0 in-flight non-inference requests\n"
     ]
    }
   ],
   "source": [
    "# Stopping Triton Server before proceeding further\n",
    "os.killpg(os.getpgid(triton_process.pid), signal.SIGTERM)  # Send the signal to all the process groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Triton FIL backend can be used for deploying tree based models trained in frameworks like LightGBM, Scikit-Learn, and cuML for fast GPU-based inference. Essentially, tree based models can now be deployed with other deep learning based models in Triton Inference Server seamlessly. Moreover, Model Analyzer utility tool can be used to profile the models and get the best deployment configuration that satisfy the deployment constraints. The trained model can then be deployed using the best configuration in Triton and Triton Client can be used for sending inference requests. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
